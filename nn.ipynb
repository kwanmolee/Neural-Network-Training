{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import MNISTtools\n",
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, Size and Feature Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtrain is: (784, 60000)\n",
      "The shape of ltrain is: (60000,)\n",
      "The size of training dataset is: 60000\n",
      "The feature dimension is: 784\n",
      "Range of xtrain pixels is: [0,255]\n"
     ]
    }
   ],
   "source": [
    "example_num = xtrain.shape[1]\n",
    "print( \"The shape of xtrain is: %s\" %(xtrain.shape,) )\n",
    "print( \"The shape of ltrain is: %s\" %(ltrain.shape,) )\n",
    "print( \"The size of training dataset is: %s\" %ltrain.size )\n",
    "print( \"The feature dimension is: %s\" %xtrain.shape[0] )\n",
    "print(\"Range of xtrain pixels is: [%s,%s]\" %(np.min(xtrain),np.max(xtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    \"\"\"\n",
    "    :param x: a collection of images\n",
    "    :type x: np.array int8\n",
    "    :return: modified version of images [-1,1]\n",
    "    :rtype: np.array float32\n",
    "    \"\"\"\n",
    "    return ((x-127.5)/127.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of index is: [0,59999]\n",
      "[28535  2425 31460 ... 47361 52534 24646]\n",
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "shuffled_ind = np.random.permutation(example_num)\n",
    "print(\"Range of index is: [%s,%s]\" %(np.min(shuffled_ind),np.max(shuffled_ind)))\n",
    "print(shuffled_ind)\n",
    "xtrain = xtrain[:,shuffled_ind]\n",
    "ltrain = ltrain[shuffled_ind]\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check output after shuffling and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of xtrain after normalization is: [-1.0,1.0]\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print(\"Range of xtrain after normalization is: [%s,%s]\" %(np.min(xtrain),np.max(xtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create holdout / validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 6000) (6000,)\n",
      "(784, 54000) (54000,)\n"
     ]
    }
   ],
   "source": [
    "valid_num = int(example_num / 10)\n",
    "xvalid = xtrain[:,(example_num - valid_num):example_num]\n",
    "lvalid = ltrain[(example_num - valid_num):example_num]\n",
    "print(xvalid.shape,lvalid.shape)\n",
    "xtrain = xtrain[:,:example_num - valid_num]\n",
    "ltrain = ltrain[:example_num - valid_num]\n",
    "print(xtrain.shape,ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label2onehot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot code dtrain[:,42] is: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] and the label it predicts is: 2\n",
      "(10, 54000) (10, 6000)\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size)) # d.shape: (9+1,60000) = (10,60000)\n",
    "    d[lbl, np.arange(0, lbl.size)] = 1\n",
    "    return d\n",
    "\n",
    "dtrain = label2onehot(ltrain)\n",
    "dvalid = label2onehot(lvalid)\n",
    "print(\"One-hot code dtrain[:,42] is: %s and the label it predicts is: %s\" %(dtrain[:,42],np.argwhere(dtrain[:,42]==1)[0][0]))\n",
    "print(dtrain.shape,dvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot2label function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(0)# return the index of max element for every column\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    return torch.exp(a - a.max(0)[0]) / (torch.exp(a - a.max(0)[0]).sum(dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    g = softmax(a)\n",
    "    product = (g * e).sum(0)\n",
    "    return g * e - product * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "xtrain = torch.from_numpy(xtrain)\n",
    "dtrain = torch.from_numpy(dtrain)\n",
    "xvalid = torch.from_numpy(xvalid)\n",
    "dvalid = torch.from_numpy(dvalid)\n",
    "ltrain = torch.from_numpy(ltrain)\n",
    "lvalid = torch.from_numpy(lvalid)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "print(device)\n",
    "xtrain,dtrain = xtrain.to(device), dtrain.to(device)\n",
    "xvalid,dvalid = xvalid.to(device), dvalid.to(device)\n",
    "ltrain,lvalid = ltrain.to(device), lvalid.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, No, mu, sig):\n",
    "\n",
    "    b = np.random.randn(No, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    #W = np.random.normal(mu, sig, (No,Ni))\n",
    "    W = np.random.randn(No, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b, W = torch.from_numpy(b), torch.from_numpy(W)\n",
    "    b, W = b.to(device), W.to(device)\n",
    "\n",
    "    return W, b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0] # 784\n",
    "No = dtrain.shape[0] # 10\n",
    "mu, sig = 0, 1e-3\n",
    "netinit = init_shallow(Ni,No, mu, sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_loss function: $E = - \\sum_{i=1}^{10}d_{i}logy_{i}$\n",
    "#### Compute averge cross-entropy loss (averaged over both training samples and vector dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y,d):\n",
    "    return - (d * torch.log(y)).sum().double() / y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk(y,d):\n",
    "    return - (d * np.log(y)).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function computes the percentage of misclassified samples between predictions and desired labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    return torch.sum(y.argmax(0)==lbl).double() / y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W = net[0]\n",
    "    b = net[1]\n",
    "    a = W.double().mm(x.double()) + b \n",
    "    y = softmax(a) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)\n",
    "#print(torch.sum(torch.argmax(yinit,0)==ltrain).double()/54000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnetinit = init_shallow(784,10, mu, sig)\\nW = netinit[0]\\nb = netinit[1]\\na = W.double().mm(torch.cat((xtrain,xvalid),1).double()) + b \\ny = softmax(a) \\nprint(eval_loss(y, torch.cat((dtrain,dvalid),1)), 'should be around .26')\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "netinit = init_shallow(784,10, mu, sig)\n",
    "W = netinit[0]\n",
    "b = netinit[1]\n",
    "a = W.double().mm(torch.cat((xtrain,xvalid),1).double()) + b \n",
    "y = softmax(a) \n",
    "print(eval_loss(y, torch.cat((dtrain,dvalid),1)), 'should be around .26')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W = net[0] # network param initialization\n",
    "    b = net[1]\n",
    "    Ni = W.shape[1] \n",
    "    No = W.shape[0]\n",
    "    #gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    # 1. Forward Process\n",
    "    a = W.double().mm(x.double()) + b # W1:(10, 54k)\n",
    "    y = softmax(a) # y:(10,54k)\n",
    "    \n",
    "    # 2. Compute Delta for Backprop\n",
    "    delta = y - d # dE = -d/y, d2:(10,54k)\n",
    "    \n",
    "    # 3. Gradient Descent\n",
    "    W -= gamma * delta.mm(x.t().double()) # gamma: learning rate\n",
    "    b -= gamma * delta.sum(1).view(No,1) #(10,1)\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ACC = []\n",
    "validation_ACC = []\n",
    "train_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, xv, dv, net, T, gamma_init):\n",
    "    \n",
    "    lbl = onehot2label(d)\n",
    "    v_lbl = onehot2label(dv)\n",
    "    \n",
    "    for t in range(T):\n",
    "        gamma = gamma_init / (1 + t / 32)\n",
    "        net = update_shallow(x, d, net, gamma) # update net\n",
    "        \n",
    "        ypred = forwardprop_shallow(x, net)\n",
    "        v_ypred = forwardprop_shallow(xv, net)\n",
    "        loss = eval_loss(ypred,d) / 10 # training loss\n",
    "        train_loss.append(loss)\n",
    "        v_loss = eval_loss(v_ypred, dv) / 10 # validation loss\n",
    "        validation_loss.append(v_loss)\n",
    "        \n",
    "        train_acc = eval_perfs(ypred,lbl)\n",
    "        train_ACC.append(train_acc)\n",
    "        validation_acc = eval_perfs(v_ypred,v_lbl)\n",
    "        validation_ACC.append(validation_acc)\n",
    "        if t % 50 == 0 or t == T-1:\n",
    "            print(\"Epoch {0}: training loss = {1}, validation loss = {2}, training acc = {3}, validation acc = {4}, gamma = {5}\".format(\n",
    "                t, loss, v_loss, train_acc, validation_acc, gamma))\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss = 1.8364758101179537, validation loss = 1.8725664600107643, training acc = 0.10366666666666667, validation acc = 0.09266666666666666, gamma = 1e-05\n",
      "Epoch 50: training loss = 0.06718858744964232, validation loss = 0.06880759835501571, training acc = 0.8726666666666667, validation acc = 0.8666666666666666, gamma = 3.902439024390244e-06\n",
      "Epoch 100: training loss = 0.055173646616544704, validation loss = 0.05646647108031322, training acc = 0.8857222222222222, validation acc = 0.8795, gamma = 2.4242424242424244e-06\n",
      "Epoch 150: training loss = 0.049965423255049074, validation loss = 0.0511580263590832, training acc = 0.8913518518518518, validation acc = 0.8848333333333332, gamma = 1.7582417582417585e-06\n",
      "Epoch 200: training loss = 0.04687602752285919, validation loss = 0.048043548958401544, training acc = 0.8951111111111111, validation acc = 0.8893333333333333, gamma = 1.3793103448275862e-06\n",
      "Epoch 250: training loss = 0.044774970222587446, validation loss = 0.04594606204379159, training acc = 0.8970925925925926, validation acc = 0.8925, gamma = 1.1347517730496454e-06\n",
      "Epoch 300: training loss = 0.043230063632138695, validation loss = 0.044415689105200884, training acc = 0.8986666666666666, validation acc = 0.8941666666666667, gamma = 9.638554216867472e-07\n",
      "Epoch 350: training loss = 0.04203455147704854, validation loss = 0.04323852003970983, training acc = 0.9003148148148148, validation acc = 0.8953333333333333, gamma = 8.376963350785341e-07\n",
      "Epoch 400: training loss = 0.0410752166556724, validation loss = 0.04229839034756404, training acc = 0.9013333333333333, validation acc = 0.8968333333333333, gamma = 7.407407407407408e-07\n",
      "Epoch 450: training loss = 0.04028415822015669, validation loss = 0.04152617899322967, training acc = 0.9022962962962963, validation acc = 0.8976666666666666, gamma = 6.639004149377594e-07\n",
      "Epoch 500: training loss = 0.03961787628124244, validation loss = 0.040877899327946034, training acc = 0.9031111111111111, validation acc = 0.8981666666666667, gamma = 6.015037593984962e-07\n",
      "Epoch 550: training loss = 0.03904706442172132, validation loss = 0.040324079241853085, training acc = 0.9040925925925926, validation acc = 0.8991666666666667, gamma = 5.498281786941582e-07\n",
      "Epoch 600: training loss = 0.038551175923627584, validation loss = 0.03984414895834965, training acc = 0.9047592592592593, validation acc = 0.9, gamma = 5.063291139240507e-07\n",
      "Epoch 650: training loss = 0.038115330944793385, validation loss = 0.039423267849377774, training acc = 0.9053518518518519, validation acc = 0.901, gamma = 4.6920821114369504e-07\n",
      "Epoch 700: training loss = 0.03772845949291264, validation loss = 0.03905043015553401, training acc = 0.9059259259259259, validation acc = 0.901, gamma = 4.3715846994535524e-07\n",
      "Epoch 750: training loss = 0.0373821364829789, validation loss = 0.038717282558674564, training acc = 0.9063518518518519, validation acc = 0.901, gamma = 4.092071611253197e-07\n",
      "Epoch 800: training loss = 0.0370698236673692, validation loss = 0.0384173580002827, training acc = 0.9068518518518518, validation acc = 0.9013333333333333, gamma = 3.8461538461538463e-07\n",
      "Epoch 850: training loss = 0.036786360709535844, validation loss = 0.03814556318530531, training acc = 0.9072962962962963, validation acc = 0.9016666666666666, gamma = 3.6281179138322e-07\n",
      "Epoch 900: training loss = 0.03652761421005043, validation loss = 0.03789782625710322, training acc = 0.9076111111111111, validation acc = 0.9016666666666666, gamma = 3.433476394849786e-07\n",
      "Epoch 950: training loss = 0.03629022992272842, validation loss = 0.0376708487359774, training acc = 0.9078518518518518, validation acc = 0.9018333333333333, gamma = 3.258655804480652e-07\n",
      "Epoch 1000: training loss = 0.036071454176090416, validation loss = 0.03746192716319541, training acc = 0.908074074074074, validation acc = 0.9021666666666667, gamma = 3.1007751937984497e-07\n",
      "Epoch 1023: training loss = 0.03597642235718012, validation loss = 0.037371255297952204, training acc = 0.9082037037037037, validation acc = 0.9026666666666666, gamma = 3.033175355450237e-07\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, xvalid, dvalid, netinit, T = 1024, gamma_init = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_acc = [t.to('cpu') for t in train_ACC]\n",
    "v_acc = [v.to('cpu') for v in validation_ACC]\n",
    "t_loss = [t.to('cpu') for t in train_loss]\n",
    "v_loss = [v.to('cpu') for v in validation_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = [t_acc,v_acc,t_loss,v_loss]\n",
    "with open(\"data\", 'wb') as sd:\n",
    "        pickle.dump(data, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = torch.from_numpy(xtest)\n",
    "ltest = torch.from_numpy(ltest)\n",
    "xtest, ltest = xtest.to(device), ltest.to(device)\n",
    "ypred_ = forwardprop_shallow(xtest,nettrain)\n",
    "test_acc = eval_perfs(ypred_,ltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8531, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network weights and average images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,dtrain = xtrain.to(device), dtrain.to(device)\n",
    "xvalid,dvalid = xvalid.to(device), dvalid.to(device)\n",
    "ltrain,lvalid = ltrain.to(device), lvalid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = nettrain[0], nettrain[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_avg = []\n",
    "for i in range(10):\n",
    "    ind = (ltrain == i).nonzero()\n",
    "    cls = xtrain[:,ind]\n",
    "    avg = torch.mean(cls,1)\n",
    "    cls_avg.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_avg = [i.reshape(28,28).to(\"cpu\") for i in cls_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_avg = []\n",
    "for i in range(10):\n",
    "    a = w[i]\n",
    "    w_avg.append(a.reshape(28,28))\n",
    "nn_avg = [i.to(\"cpu\") for i in w_avg]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data1 = [c_avg,nn_avg]\n",
    "with open(\"avgimg\", 'wb') as sd:\n",
    "        pickle.dump(data1, sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow1(x, d, xv, dv, net, T, gamma_init):\n",
    "    \n",
    "    lbl = onehot2label(d)\n",
    "    v_lbl = onehot2label(dv)\n",
    "    \n",
    "    for t in range(T):\n",
    "        gamma = gamma_init / (1 + t / 32)\n",
    "        net = update_shallow(x, d, net, gamma) # update net\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9080, device='cuda:0', dtype=torch.float64) tensor(0.8995, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.9073, device='cuda:0', dtype=torch.float64) tensor(0.8998, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.9079, device='cuda:0', dtype=torch.float64) tensor(0.8990, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.9075, device='cuda:0', dtype=torch.float64) tensor(0.9000, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.9083, device='cuda:0', dtype=torch.float64) tensor(0.9017, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.8745, device='cuda:0', dtype=torch.float64) tensor(0.8762, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.7148, device='cuda:0', dtype=torch.float64) tensor(0.7208, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "gamma_set = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "training_acc = []\n",
    "valid_acc = []\n",
    "for g in gamma_set:\n",
    "    Ni = xtrain.shape[0] # 784\n",
    "    No = dtrain.shape[0] # 10\n",
    "    mu, sig = 0, 1e-3\n",
    "    netinit = init_shallow(Ni,No, mu, sig)\n",
    "    nettrain = backprop_shallow1(xtrain, dtrain, xvalid, dvalid, netinit, T = 1024, gamma_init = g)\n",
    "    y = forwardprop_shallow(xtrain, nettrain)\n",
    "    v_y = forwardprop_shallow(xvalid, nettrain)\n",
    "    train_acc = eval_perfs(y,ltrain)\n",
    "    validation_acc = eval_perfs(v_y,lvalid)\n",
    "    print(train_acc,validation_acc)\n",
    "    training_acc.append(train_acc)\n",
    "    valid_acc.append(validation_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.9038, dtype=torch.float64), tensor(0.9071, dtype=torch.float64), tensor(0.9081, dtype=torch.float64), tensor(0.9064, dtype=torch.float64), tensor(0.9071, dtype=torch.float64), tensor(0.8716, dtype=torch.float64), tensor(0.6947, dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [i.to('cpu') for i in training_acc]\n",
    "a2 = [i.to('cpu') for i in valid_acc]\n",
    "data2 = [a1, a2]\n",
    "with open(\"eta\",\"wb\") as f:\n",
    "    pickle.dump(data2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>20</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.1 Implement the Backpropagation based on SGD/Minibatch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, v, dv, net, T, B=64, gamma_init=.05):\n",
    "\n",
    "    N = x.shape[1] # sample number\n",
    "    NB = int((N + B - 1)/B) # = N/B + 1 - 1/B \n",
    "    lbl = onehot2label(d)\n",
    "    v_lbl = onehot2label(dv)\n",
    "    \n",
    "    for t in range(0, T):\n",
    "        shuffled_indices = torch.randperm(N)\n",
    "        gamma = gamma_init / (1 + t / 8)\n",
    "        \n",
    "        # mini-batch learning with NB updates\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l : min(B*(l+1), N)] # the indices of training samples == the indices of columns\n",
    "            net = update_shallow(x[:,minibatch_indices],d[:,minibatch_indices],net,gamma) # update net\n",
    "        \n",
    "        # report the loss and training error rates every epoch\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        y_ = forwardprop_shallow(v, net)\n",
    "        loss = eval_loss(y,d) / 10\n",
    "        t_risk.append(loss)\n",
    "        v_loss = eval_loss(y_,dv) / 10\n",
    "        v_risk.append(v_loss)\n",
    "        train_acc = eval_perfs(y, lbl)\n",
    "        t_acc.append(train_acc)\n",
    "        valid_acc = eval_perfs(y_, v_lbl)\n",
    "        v_acc.append(valid_acc)\n",
    "        print(\"Epoch {0}: train_loss= {1}, valid_loss= {2}, train_acc= {3}, valid_acc= {4}\".format(t, loss, v_loss, train_acc, valid_acc))\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss= 0.03319656546053445, valid_loss= 0.03089148802476522, train_acc= 0.9055925925925926, valid_acc= 0.9099999999999999\n",
      "Epoch 1: train_loss= 0.031966146467132917, valid_loss= 0.030230005567258494, train_acc= 0.9056481481481481, valid_acc= 0.9053333333333333\n",
      "Epoch 2: train_loss= 0.030425697646291463, valid_loss= 0.029620633154275516, train_acc= 0.9126851851851852, valid_acc= 0.9103333333333333\n",
      "Epoch 3: train_loss= 0.03059660700616519, valid_loss= 0.029648231300550804, train_acc= 0.9107962962962963, valid_acc= 0.9119999999999999\n",
      "Epoch 4: train_loss= 0.028257758341493147, valid_loss= 0.027407534517183862, train_acc= 0.9192222222222222, valid_acc= 0.9213333333333333\n",
      "Epoch 5: train_loss= 0.028152146630412156, valid_loss= 0.02747860653541713, train_acc= 0.9197407407407407, valid_acc= 0.9208333333333333\n",
      "Epoch 6: train_loss= 0.028543575987236426, valid_loss= 0.028148234668459178, train_acc= 0.9197777777777778, valid_acc= 0.9196666666666666\n",
      "Epoch 7: train_loss= 0.02727554238556509, valid_loss= 0.02704289088065114, train_acc= 0.9232222222222222, valid_acc= 0.9223333333333333\n"
     ]
    }
   ],
   "source": [
    "g = 1e-3\n",
    "mu, sig = 0, 1e-3\n",
    "t_acc, v_acc = [], []\n",
    "t_risk, v_risk = [], []\n",
    "Ni = xtrain.shape[0] # 784\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, No, mu, sig)\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, xvalid, dvalid, netinit, T = 8, B=64, gamma_init = g)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data4 = [t_acc,v_acc,t_risk,v_risk]\n",
    "data4_ = []\n",
    "for i in range(4):\n",
    "    d = data4[i]\n",
    "    d_ = [x.to(\"cpu\") for x in d]\n",
    "    data4_.append(d_)\n",
    "\n",
    "with open(\"batchdata\",\"wb\") as f:\n",
    "    pickle.dump(data4_,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = torch.from_numpy(xtest)\n",
    "ltest = torch.from_numpy(ltest)\n",
    "xtest, ltest = xtest.to(device), ltest.to(device)\n",
    "ybatch = forwardprop_shallow(xtest,netminibatch)\n",
    "test_acc = eval_perfs(ybatch,ltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8543, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
