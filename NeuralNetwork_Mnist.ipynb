{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>----------------------------- Getting Started -----------------------------</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>---------------------------- Read MNIST Data ----------------------------</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "#help(MNISTtools.load)\n",
    "#help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>1</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Store the images and lables from the training datasets into 2 variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Shape, Size and Feature Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtrain is: (784, 60000)\n",
      "The shape of ltrain is: (60000,)\n",
      "The size of training dataset is: 60000\n",
      "The feature dimension is: 784\n"
     ]
    }
   ],
   "source": [
    "print( \"The shape of xtrain is: %s\" %(xtrain.shape,) )\n",
    "print( \"The shape of ltrain is: %s\" %(ltrain.shape,) )\n",
    "print( \"The size of training dataset is: %s\" %ltrain.size )\n",
    "print( \"The feature dimension is: %s\" %xtrain.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float: left}\n",
       "</style>\n",
       "<style> \n",
       "table td, table th, table tr {text-align:center !important;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float: left}\n",
    "</style>\n",
    "<style> \n",
    "table td, table th, table tr {text-align:center !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable     |Property | Value      |\n",
    "|:---|:---|:---|\n",
    "|xtrain|shape|(784,60000)|\n",
    "|ltrain|shape|(60000,)|\n",
    "|training dataset|size|60000|\n",
    "|training dataset|Feature dimension|784|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>2</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Display the image (index = 42) and check its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image with index 42:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of image with index 42: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"The image with index 42:\")\n",
    "MNISTtools.show(xtrain[:, 42])\n",
    "print(\"The label of image with index 42: %s\" %ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Range of xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of xtrain is: [0,255]\n"
     ]
    }
   ],
   "source": [
    "print(\"Range of xtrain is: [%s,%s]\" %(np.min(xtrain),np.max(xtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Type of xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of xtrain is: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of xtrain is: %s\" %type(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>4</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Function Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    \"\"\"\n",
    "    :param x: a collection of images\n",
    "    :type x: np.array int8\n",
    "    :return: modified version of images [-1,1]\n",
    "    :rtype: np.array float32\n",
    "    \"\"\"\n",
    "    return ((x-127.5)/127.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 check Output of Functin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of xtrain after normalization is: [-1.0,1.0]\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print(\"Range of xtrain after normalization is: [%s,%s]\" %(np.min(xtrain),np.max(xtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>5</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Complete label2onehot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot code dtrain[:,42] is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] and the label it predicts is: 7\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    \"\"\"\n",
    "    : param lbl: labels of images \n",
    "    : type lbl: np.array, shape(60000,)\n",
    "    : return: one-hot codes \n",
    "    : rtype: np.array, shape(10,60000)\n",
    "    \"\"\"\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size)) # d.shape: (9+1,60000) = (10,60000)\n",
    "    d[lbl, np.arange(0, lbl.size)] = 1\n",
    "    return d\n",
    "\n",
    "dtrain = label2onehot(ltrain)\n",
    "print(\"One-hot code dtrain[:,42] is: %s and the label it predicts is: %s\" %(dtrain[:,42],np.argwhere(dtrain[:,42]==1)[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltrain[42] is: 7\n",
      "dtrain[:,42] corresponds to ltrain[42]: True\n"
     ]
    }
   ],
   "source": [
    "print(\"ltrain[42] is: %s\" %ltrain[42])\n",
    "print(\"dtrain[:,42] corresponds to ltrain[42]: %s\" %(ltrain[42]==np.argwhere(dtrain[:,42]==1)[0][0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float: left}\n",
       "</style>\n",
       "<style> \n",
       "table td, table th, table tr {text-align:center !important;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float: left}\n",
    "</style>\n",
    "<style> \n",
    "table td, table th, table tr {text-align:center !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable|Value |\n",
    "|---|---|\n",
    "|dtrain[:,42]|[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]|\n",
    "|ltrain[42]|7|\n",
    "|dtrain corresponds to ltrain| Yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>6</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Complete onehot2label function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    \"\"\"\n",
    "    : param d: one-hot codes\n",
    "    : type d: np.array, shape(10,n) - n: number of samples\n",
    "    : return: corresponding labels of one-hot codes\n",
    "    : rtype: np.array, shape(n,) - n: number of samples\n",
    "    \"\"\"\n",
    "    lbl = d.argmax(axis=0)# return the index of max element for every column\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 check the function by ltrain == onehot2label(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltrain == onehot2label(dtrain): True\n"
     ]
    }
   ],
   "source": [
    "print(\"ltrain == onehot2label(dtrain): %s\" %all(ltrain == onehot2label(dtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>---------------------------- Activation Functions ----------------------------</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>7</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 softmax function computes $g(\\alpha)_{i} = \\frac{exp(\\alpha_{i}-M)}{\\sum_{j=1}^{10}exp(\\alpha_{j}-M)}$ where $ M = \\mathop{max}\\limits_{j=1,...,10}(\\alpha_{j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    : compute softmax activation function\n",
    "    : param a: vectors in R**10\n",
    "    : type a: np.array, shape (10,60000) \n",
    "    : return: vectors with each element yi represents the probability in class i\n",
    "    : type y: np.array, shape (10,60000)\n",
    "    \"\"\"\n",
    "    return np.exp(a - a.max(axis = 0)) / (np.exp(a - a.max(axis = 0)).sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> 8 </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Proof of $\\frac{ \\partial g(\\alpha)_{i}}{\\partial \\alpha_{i}} = g(\\alpha)_{i}(1-g(\\alpha)_{i})$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "{g(\\alpha)_{i}} &= \\frac {exp(\\alpha_{i})} {\\sum_{j=1}^{10} exp(\\alpha_{j})} \\\\\n",
    "{\\frac{ \\partial g(\\alpha)_{i}}{\\partial \\alpha_{i}} }& = \n",
    "\\frac{exp(\\alpha_{i}) \\times \\sum_{j=1}^{10} exp(\\alpha_{j}) - exp(\\alpha_{i}) \\times exp(\\alpha_{j})|_{j=i} }\n",
    "{(\\sum_{j=1}^{10} exp(\\alpha_{j}))^2} \\\\\n",
    "&= \\frac{exp(\\alpha_{i})}{\\sum_{j=1}^{10} exp(\\alpha_{j})} - \\frac{exp(\\alpha_{i}^{2})}{(\\sum_{j=1}^{10} exp(\\alpha_{j}))^2} = g(a)_{i} - (g(a)_{i})^2 \\\\\n",
    "&= g(\\alpha)_{i}(1-g(\\alpha)_{i}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>9</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Proof of $\\frac{ \\partial g(\\alpha)_{i}}{\\partial \\alpha_{j}} = - g(\\alpha)_{i}g(\\alpha)_{j}$ for $j \\not= i$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "{\n",
    "g(\\alpha)_{i}\n",
    "}& = \\frac {exp(\\alpha_{i})}{\\sum_{j=1}^{10} exp(\\alpha_{j})}\\\\\n",
    "{\n",
    "\\frac{ \\partial g(\\alpha)_{i}}{\\partial \\alpha_{j}}\n",
    "}& = - \\frac{exp(\\alpha_{i}) \\times exp(\\alpha_{j})}{(\\sum_{j=1}^{10} exp(\\alpha_{j}))^2} \\\\\n",
    "& = - \\frac{exp(\\alpha_{i})}{\\sum_{j=1}^{10} exp(\\alpha_{j})} \\times \\frac{exp(\\alpha_{j})}{\\sum_{j=1}^{10} exp(\\alpha_{j})} \\\\\n",
    "&= - g(\\alpha)_{i}g(\\alpha)_{j} for j \\not= i \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>10</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 softmaxp function computes $\\delta = g(\\alpha)\\otimes e - <g(\\alpha),e>g(\\alpha)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    \"\"\"\n",
    "    : compute delta for backprop algorithm\n",
    "    : param a: output by output layers\n",
    "    : type a: np.array, shape(10,60k)\n",
    "    : param e: d(E), E: cross-entropy\n",
    "    : type e: np.array, shape(10,60k)\n",
    "    : return: delta\n",
    "    : rtype: np.array, shape(10,60k)\n",
    "    \"\"\"\n",
    "    g = softmax(a)\n",
    "    product = (g * e).sum(axis = 0)\n",
    "    return g * e - product * g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>11</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Check function softmaxp $\\delta = \\frac{\\partial g(\\alpha)}{\\partial \\alpha}\\times e = \\mathop{lim}\\limits_{\\epsilon \\rightarrow 0}\\frac{g(\\alpha + \\epsilon e) -g(\\alpha)}{\\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.985804833788473e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step: epsilon\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e) # delta\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Five experiement records of real_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Time|Value|\n",
    "|---|---|\n",
    "|1st|5.28644217250807e-07|\n",
    "|2nd|5.101713352304516e-07|\n",
    "|3rd|4.885335947948998e-07|\n",
    "|4th|4.853878161851958e-07|\n",
    "|5th|4.860601104238716e-07|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>12</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 ReLU$(\\alpha)_{i} = max(\\alpha_{i},0)$ and ReLUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    \"\"\"\n",
    "    : relu(a) = a if a>=0 else 0\n",
    "    : param a: input of hidden layers\n",
    "    : type a: np.array, shape(64,60k)\n",
    "    : rtype: np.array, shape(64,60k)\n",
    "    \"\"\"\n",
    "    return (a>0) * a \n",
    "def relup(a, e):\n",
    "    \"\"\"\n",
    "    : relup(a,e) = a * e if a>=0 else 0\n",
    "    : param a: delta 2\n",
    "    : type a: \n",
    "    : type a: np.array, shape(64,60k)\n",
    "    : return: delta 1\n",
    "    : rtype: np.array, shape(64,60k) \n",
    "    \"\"\"\n",
    "    return (a>0) * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>---------------------------- Back Propogation ----------------------------</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>13</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 He and Xavier initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    \"\"\"\n",
    "    : param Ni: dimension of input layer = 784\n",
    "    : type Ni: int\n",
    "    : param Nh: number of hidden layer units = 64\n",
    "    : type Nh: int\n",
    "    : param No: number of output layer units = 10\n",
    "    : type No: int\n",
    "    : return: weights and biases\n",
    "    : rtype: tuple(int)\n",
    "    \"\"\"\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0] # 784\n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "#saved_init_net = copy.deepcopy(netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable|$W_{1}$|$b_{1}$|$W_{2}$|$b_{2}$|\n",
    "|---|---|---|---|---|\n",
    "|Shape|(64, 784)|(64,1)|(10,64)|(10, 1)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>14</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 forwardprop_shallow function: evaluate the prediction of initial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    \"\"\"\n",
    "    : implement forwardprop and return predictions \n",
    "    : input: x-> a1 = W1x+b1 -> hidden layer: h1 = relu(a1), a2 = W2h1+b2 -> output: y = softmax(a2)\n",
    "    : return: predictions\n",
    "    : rtype: np.array, shape (10,60000)\n",
    "    \"\"\"\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1 # input of hidden layer\n",
    "    h1 = relu(a1) # output of hidden layer, activation function: ReLU\n",
    "    a2 = W2.dot(h1) + b2 # input of output layer\n",
    "    y = softmax(a2) # output, activation function: softmax\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable|$a_{1}$|$h_{1}$|$a_{2}$|$y$|\n",
    "|---|---|---|---|---|\n",
    "|Shape|(64, 60000)|(64, 60000)|(10, 60000)|(10, 60000)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>15</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1 eval_loss function: $E = - \\sum_{i=1}^{10}d_{i}logy_{i}$\n",
    "#### Compute averge cross-entropy loss (averaged over both training samples and vector dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y,d):\n",
    "    \"\"\"\n",
    "    : compute the average cross-entropy loss \n",
    "    : param y: predictions \n",
    "    : type y: np.array, shape (10,n) - n: number of samples\n",
    "    : param d: one-hot codes\n",
    "    : type d: np.array, shape (10,n) - n: number of samples\n",
    "    \"\"\"\n",
    "    return - (d * np.log(y)).sum() / d.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27009475571637825 should be around .26\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Cross-entropy H|Standard|Difference|\n",
    "|---|---|---|\n",
    "|0.2550986268633796|0.26|0.004901373136620424|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>16</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.1 Function computes the percentage of misclassified samples between predictions and desired labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    \"\"\"\n",
    "    : compute the misclassification percentage of training \n",
    "    : param y: predictions\n",
    "    : type y: np.array, shape(10,n) -n: number of samples\n",
    "    : param lbl: desired labels\n",
    "    : type lbl: np.array, shape(n,) -n: number of samples\n",
    "    : return: percentage of misclassified samples\n",
    "    : rtype: float\n",
    "    \"\"\"\n",
    "    #print(\"Number of misclassified samples: %s\" %len(np.argwhere(onehot2label(y) != lbl)))\n",
    "    return np.sum(np.not_equal(np.argmax(y,axis=0),lbl))*1.0/lbl.size\n",
    "    #return len(np.argwhere(onehot2label(y) != lbl))/lbl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of misclassified samples between y and lbl: 0.9056666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of misclassified samples between y and lbl: %s\" %eval_perfs(yinit,ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Misclassified|Samples|\n",
    "|---|---|\n",
    "|Number|54045|\n",
    "|Percentage|0.90075|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>17</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of $(\\triangledown_{y}E)_{i} = - \\frac{d_{i}}{y_{i}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "{E} &= - \\sum_{i=1}^{10}d_{i}logy_{i} \\\\\n",
    "{(\\triangledown_{y}E)_{i}} &= -d_{i}\\triangledown_{y}(\\sum_{i=1}^{10}logy_{i}) \\\\\n",
    "&= - \\frac{d_{i}}{y_{i}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0] # network param initialization\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1] # feture dimension, hidden units, output neurons\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    # 1. Forward Process\n",
    "    a1 = W1.dot(x) + b1 # W1:(64, 784), b1:(64,1), a1:(64,1)\n",
    "    h1 = relu(a1) # a1:(64, 60k), h1:(64,60k)\n",
    "    a2 = W2.dot(h1) + b2 # W2:(10,64), b2:(10,1), a2:(10,60k)\n",
    "    y = softmax(a2) # y:(10,60k)\n",
    "    \n",
    "    # 2. Compute Delta for Backprop\n",
    "    d2 = softmaxp(a2, -d/y) # dE = -d/y, d2:(10,60k)\n",
    "    d1 = relup(a1, W2.T.dot(d2)) # d1:(64,60k)\n",
    "    \n",
    "    # 3. Gradient Descent\n",
    "    W2 -= gamma * d2.dot(h1.T) # gamma: learning rate\n",
    "    W1 -= gamma * d1.dot(x.T)\n",
    "    b2 -= gamma * d2.sum(axis=1).reshape(No,1) #(10,1)\n",
    "    b1 -= gamma * d1.sum(axis=1).reshape(Nh,1) #(64,1)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>18</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    \"\"\"\n",
    "    : param T: the iteration times\n",
    "    : type T: int\n",
    "    : return updated network and print the training loss and percentage of training errors for each iteration\n",
    "    \"\"\"\n",
    "    lbl = onehot2label(d)\n",
    "    if T < 50: \n",
    "        for t in range(T):\n",
    "            net = update_shallow(x,d,net,gamma) # update net\n",
    "\n",
    "            ypred = forwardprop_shallow(x,net)\n",
    "            loss = eval_loss(ypred,d) # training loss\n",
    "            train_err = eval_perfs(ypred,lbl)\n",
    "            print(\"Iteration %s: loss= %s, training error rate: %s\" %(t, loss, train_err))\n",
    "    else: # do not report the loss and training errors every iteration if T is too large\n",
    "        for t in range(T):\n",
    "            net = update_shallow(x,d,net,gamma) # update net\n",
    "            if t % 5 == 0 or t == T-1:\n",
    "                ypred = forwardprop_shallow(x,net)\n",
    "                loss = eval_loss(ypred,d) # training loss\n",
    "                train_err = eval_perfs(ypred,lbl)\n",
    "                print(\"Iteration %s: loss= %s, training error rate= %s\" %(t, loss, train_err))\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>18.1: Performe Network Update with Different Numbers of T (Iteration Times)</center><h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.1 T = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss= 0.23362577870570622, training error rate: 0.8588666666666667\n",
      "Iteration 1: loss= 0.22092386006196554, training error rate: 0.77365\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.2 T = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss= 0.23213637190953895, training error rate: 0.8124166666666667\n",
      "Iteration 1: loss= 0.2201284810041138, training error rate: 0.8227166666666667\n",
      "Iteration 2: loss= 0.2115048430069544, training error rate: 0.7004833333333333\n",
      "Iteration 3: loss= 0.2043467191552892, training error rate: 0.6803666666666667\n",
      "Iteration 4: loss= 0.1976570604996038, training error rate: 0.6291\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0] # Remember to initiate the net after one learning \n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.3 T = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss= 0.22765905993787255, training error rate: 0.86175\n",
      "Iteration 1: loss= 0.21672011432335864, training error rate: 0.7691833333333333\n",
      "Iteration 2: loss= 0.20843756351587925, training error rate: 0.7136666666666667\n",
      "Iteration 3: loss= 0.2011236476680443, training error rate: 0.6487\n",
      "Iteration 4: loss= 0.19427061880926336, training error rate: 0.6048333333333333\n",
      "Iteration 5: loss= 0.1876926321291342, training error rate: 0.5628166666666666\n",
      "Iteration 6: loss= 0.181340796423651, training error rate: 0.5299166666666667\n",
      "Iteration 7: loss= 0.17522895661886942, training error rate: 0.5000166666666667\n",
      "Iteration 8: loss= 0.1693691545961573, training error rate: 0.4741666666666667\n",
      "Iteration 9: loss= 0.1637719113915629, training error rate: 0.45155\n",
      "Iteration 10: loss= 0.15845667474963937, training error rate: 0.4308166666666667\n",
      "Iteration 11: loss= 0.15342266864908397, training error rate: 0.41235\n",
      "Iteration 12: loss= 0.1486729644261387, training error rate: 0.3973\n",
      "Iteration 13: loss= 0.14418166716582992, training error rate: 0.38058333333333333\n",
      "Iteration 14: loss= 0.13997837333676943, training error rate: 0.37006666666666665\n",
      "Iteration 15: loss= 0.136034552881698, training error rate: 0.3542166666666667\n",
      "Iteration 16: loss= 0.13244848916961402, training error rate: 0.35041666666666665\n",
      "Iteration 17: loss= 0.12916982432494634, training error rate: 0.3368333333333333\n",
      "Iteration 18: loss= 0.12651776230486483, training error rate: 0.3444833333333333\n",
      "Iteration 19: loss= 0.12421417266790381, training error rate: 0.3357833333333333\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0] # Remember to initiate the net after one learning \n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.4 T = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss= 0.24367577282936234, training error rate= 0.8844166666666666\n",
      "Iteration 5: loss= 0.18726948753518505, training error rate= 0.54495\n",
      "Iteration 10: loss= 0.15506176135592672, training error rate= 0.40405\n",
      "Iteration 15: loss= 0.1295561902892838, training error rate= 0.3235\n",
      "Iteration 20: loss= 0.11134611374674619, training error rate= 0.27266666666666667\n",
      "Iteration 25: loss= 0.09914059248802207, training error rate= 0.24451666666666666\n",
      "Iteration 30: loss= 0.09618422658993153, training error rate= 0.2781166666666667\n",
      "Iteration 35: loss= 0.08932679972300604, training error rate= 0.24893333333333334\n",
      "Iteration 40: loss= 0.07707560884984281, training error rate= 0.20933333333333334\n",
      "Iteration 45: loss= 0.0710303578506613, training error rate= 0.18128333333333332\n",
      "Iteration 49: loss= 0.06716689387148511, training error rate= 0.1717\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0] # Remember to initiate the net after one learning \n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.5 T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss= 0.23664206164085744, training error rate= 0.8762833333333333\n",
      "Iteration 5: loss= 0.19013780113928733, training error rate= 0.5958333333333333\n",
      "Iteration 10: loss= 0.15969098308303314, training error rate= 0.41596666666666665\n",
      "Iteration 15: loss= 0.14235385410771514, training error rate= 0.3989666666666667\n",
      "Iteration 20: loss= 0.12071983712074824, training error rate= 0.32253333333333334\n",
      "Iteration 25: loss= 0.10328655898543378, training error rate= 0.27403333333333335\n",
      "Iteration 30: loss= 0.09179158068297094, training error rate= 0.2336\n",
      "Iteration 35: loss= 0.08374088190646001, training error rate= 0.22261666666666666\n",
      "Iteration 40: loss= 0.07652583295047381, training error rate= 0.19813333333333333\n",
      "Iteration 45: loss= 0.07138363593555792, training error rate= 0.18871666666666667\n",
      "Iteration 50: loss= 0.06688057942008423, training error rate= 0.17371666666666666\n",
      "Iteration 55: loss= 0.06355846790293433, training error rate= 0.17003333333333334\n",
      "Iteration 60: loss= 0.06061873252760616, training error rate= 0.15836666666666666\n",
      "Iteration 65: loss= 0.058233139304317255, training error rate= 0.15641666666666668\n",
      "Iteration 70: loss= 0.056101452718339946, training error rate= 0.14848333333333333\n",
      "Iteration 75: loss= 0.05427885811728329, training error rate= 0.14603333333333332\n",
      "Iteration 80: loss= 0.052655681194740005, training error rate= 0.14023333333333332\n",
      "Iteration 85: loss= 0.05123783591173054, training error rate= 0.13806666666666667\n",
      "Iteration 90: loss= 0.049981410543844744, training error rate= 0.13401666666666667\n",
      "Iteration 95: loss= 0.048866619800864125, training error rate= 0.13233333333333333\n",
      "Iteration 99: loss= 0.048060694937516565, training error rate= 0.13043333333333335\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0] # 784\n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.6 Conclusion\n",
    "|Itertation T|2|5|20|50|100\n",
    "|---|---|---|---|---|---|\n",
    "|Final Loss|0.2209|0.1977|0.1242|0.0672|0.0481|\n",
    "|Final Training Error|0.7737|0.6291|0.3358|0.1717|0.1304|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training loss and training errors both decrease as the iteration T inreases, although there is occasionally some fluctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>19</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1 Load test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = normalize_MNIST_images(xtest) \n",
    "dtest = label2onehot(ltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtest is: (784, 10000)\n",
      "The shape of ltest is: (10000,)\n",
      "The size of testing dataset is: 10000\n"
     ]
    }
   ],
   "source": [
    "print( \"The shape of xtest is: %s\" %(xtest.shape,) )\n",
    "print( \"The shape of ltest is: %s\" %(ltest.shape,) )\n",
    "print( \"The size of testing dataset is: %s\" %ltest.size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.2 Evaluate the performance of network on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set: loss= 0.046197535002024254, testing error rate: 0.1222\n"
     ]
    }
   ],
   "source": [
    "# use the trained net acquired instead of the initial one\n",
    "ypred = forwardprop_shallow(xtest,nettrain)\n",
    "test_loss = eval_loss(ypred,dtest)\n",
    "test_err = eval_perfs(ypred,ltest)\n",
    "print(\"Testing set: loss= %s, testing error rate: %s\"%(test_loss,test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We can see that the testing loss and testing error rate are both lower than those of training with different numbers of iteration. Considering that the testing error rate has dropped below 0.13 to 0.1222, approximately to the level of 0.12, we can conclude that the network performs well on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>20</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.1 Implement the Backpropagation based on SGD/Minibatch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss= 0.0293734800233284, training error rate= 0.08416666666666667\n",
      "Epoch 1: loss= 0.022940207279313635, training error rate= 0.06593333333333333\n",
      "Epoch 2: loss= 0.019870220360254365, training error rate= 0.05558333333333333\n",
      "Epoch 3: loss= 0.016840284834467454, training error rate= 0.04881666666666667\n",
      "Epoch 4: loss= 0.014973666292027843, training error rate= 0.04318333333333333\n"
     ]
    }
   ],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    \"\"\"\n",
    "    : x - training data; d - one-hot codes of training data; net - initial neural network; T - ecpoch; gamma - learning rate \n",
    "    : param B: mini batches / the size of random blocks\n",
    "    : type B: int\n",
    "    : return: trained neural network\n",
    "    \"\"\"\n",
    "    N = x.shape[1] # sample number\n",
    "    NB = int((N + B - 1)/B) # = N/B + 1 - 1/B \n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        \n",
    "        # mini-batch learning with NB updates\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l : min(B*(l+1), N)] # the indices of training samples == the indices of columns\n",
    "            net = update_shallow(x[:,minibatch_indices],d[:,minibatch_indices],net,gamma) # update net\n",
    "        \n",
    "        # report the loss and training error rates every epoch\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        loss = eval_loss(y,d)\n",
    "        train_err = eval_perfs(y,lbl)\n",
    "        print(\"Epoch %s: loss= %s, training error rate= %s\" %(t, loss, train_err))\n",
    "    \n",
    "    return net\n",
    "\n",
    "Ni = xtrain.shape[0] # 784\n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>21</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.1 Compare the performance of this new network on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Learning Testing set: loss= 0.01540219400400598, testing error rate: 0.0453\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,netminibatch)\n",
    "test_loss = eval_loss(ypred,dtest)\n",
    "test_err = eval_perfs(ypred,ltest)\n",
    "print(\"Minibatch Learning Testing set: loss= %s, testing error rate: %s\"%(test_loss,test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not only is the Minibatch learning more efficient since its training procedure gets faster, but also it generates better results as the  loss and error rates are both lower in this case. Hence, based on the experiment results for this task, we generally reach a win-win by using Minibatch learning with fast training time and decent training performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
